编译命令：arm-linux-gcc yuyv.c lcd.c -o yuyv  -I libjpeg/ -L libjpeg/ -ljpeg


一，什么是v4l2？
Video for Linux two(Video4Linux2)简称V4L2，是V4L的改进版。
V4L2是linux操作系统下用于采集图片、视频和音频数据的API接口，
配合适当的视频采集设备和相应的驱动程序，可以实现图片、视频、音频等的采集。
在远程会议、可视电话、视频监控系统和嵌入式多媒体终端中都有广泛的应用。

在Linux下，所有外设都被看成一种特殊的文件，成为“设备文件”，可以象访问普通文件一样对其进行读写。
一般来说，采用V4L2驱动的摄像头设备文件是/dev/v4l/video0。为了通用，可以建立一个到/dev/video0的链接。
V4L2支持两种方式来采集图像：内存映射方式(mmap)和直接读取方式(read)。
V4L2在include/linux/videodev.h文件中定义了一些重要的数据结构，在采集图像的过程中，
就是通过对这些数据的操作来获得最终的图像数据。Linux系统V4L2的能力可在Linux内核编译阶段配置，
默认情况下都有此开发接口。V4L2从Linux 2.5.x版本的内核中开始出现。

V4L2规范中不仅定义了通用API元素(Common API Elements)，图像的格式(Image Formats)，输入/输出方法(Input/Output)，还定义了Linux内核驱动处理视频信息的一系列接口(Interfaces)，这些接口主要有：
视频采集接口——Video Capture Interface;
视频输出接口—— Video Output Interface;
视频覆盖/预览接口——Video Overlay Interface;
视频输出覆盖接口——Video Output Overlay Interface;
解码接口——Codec Interface。
　　
二、应用程序通过V4L2进行视频采集的原理
V4L2支持内存映射方式(mmap)和直接读取方式(read)来采集数据，前者一般用于连续视频数据的采集，后者常用于静态图片数据的采集，
应用程序通过V4L2接口采集视频数据分为五个步骤：
首先，打开视频设备文件，进行视频采集的参数初始化，通过V4L2接口设置视频图像的采集窗口、采集的点阵大小和格式;
其次，申请若干视频采集的帧缓冲区，并将这些帧缓冲区从内核空间映射到用户空间，便于应用程序读取/处理视频数据;
第三，将申请到的帧缓冲区在视频采集输入队列排队，并启动视频采集;
第四，驱动开始视频数据的采集，应用程序从视频采集输出队列取出帧缓冲区，处理完后，将帧缓冲区重新放入视频采集输入队列，循环往复采集连续的视频数据;
五，停止视频采集。

 我们现在平时所看到的视频，从摄像头数据采集到最后显示频显示出来，初略的归纳经过了下面的几个环节：摄像头采集图像数据->将图像数据进行压缩->将压缩的图像数据封装成视频格式。视频格式文件->去除视频格式封装->解压缩图像数据->得到一帧图像的数据->显示器显示图像。

    这里只介绍摄像头数据的采集以及将采集到的数据压缩成视频流的一些操作。

    摄像头输出三类数据：YUV，RGB，JPEG。YUV是大多数视频编码所需要的数据格式，比如X264编码器编码h264视频格式，它需要独立分离Y,U,V三个分量上的数据。RGB 数据可以直接发送到显示屏显示，通过R(红),G（绿）,B(蓝),三色可以组合成其它的各种颜色。显示屏最后都是通过RGB数据格式把颜色显示出来。JPEG是一种经过压缩的图片格式，为了减少数据量，在有些摄像头内部它就可以进行图片的压缩，一帧JPEG格式的数据直接保存成文件就是一张JPG的图片，把一系列JPEG的图片依循存储就是MJPEG格式的视频格式了。与x264编码相比，MJPEG编码清晰度较高，但是数据量较大(压缩率低，失真率也低)

YUV数据分很多种，比如YUV420，YUV422，YUV422P等等，对于有些视频编码，它只能输入YUV数据。比如X264编码器编码h264视频格式数据，它需要输入Y,U,V三个分量的具体数据。一帧图像的YUV数据并不能像图片一样查看，需要专门的YUV查看软件查看，比如pYUV。YUV数据的具体定义网上有很多资料这里不再介绍。



























